{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGna45Mfeadd"
   },
   "source": [
    "# **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AO8bUaKYePrR",
    "outputId": "0400cf13-155e-4283-e58b-7d8f966a1073",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pymysql\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB7cPxDUa7Z9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnNxQmSTD9qL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OqorOpefiIs"
   },
   "source": [
    "# **Login**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fHd32_K9fkFM",
    "outputId": "316d9996-5a8b-4a96-ed7b-a5e538e7ba68",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pwd = getpass('password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dG6M8c5UITH5",
    "outputId": "103841e3-979a-4f63-a90a-6040f903a854",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDAX_D15fmi2"
   },
   "source": [
    "# **Database Connection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1CYhUIGTgzp"
   },
   "source": [
    "The dataset used is provided by Sofvie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRVqmnA3ZYbW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatabaseConnection:\n",
    "    def __init__(self, pwd):\n",
    "        self.pwd = pwd\n",
    "        self.url = f\"mysql+pymysql://student1:{self.pwd}@159.203.63.26/sofvie_test\"\n",
    "        self.con = None\n",
    "\n",
    "    def initialize(self):\n",
    "        if self.con is None:\n",
    "            engine = create_engine(self.url)\n",
    "            self.con = engine.connect()\n",
    "\n",
    "    def run_sql(self, sql):\n",
    "        self.initialize()\n",
    "        data = pd.read_sql(sql, con=self.con)\n",
    "        return data\n",
    "\n",
    "    def close(self):\n",
    "        self.con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbROwznCZdN-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "database_connection = DatabaseConnection(pwd)\n",
    "database_connection.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZE4VKiCAZnlD"
   },
   "source": [
    "#**Database Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJNltPsOZ2j0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TableNode:\n",
    "    def __init__(self, selected_columns, table_name, table_as_name, children):\n",
    "        self.selected_columns = selected_columns\n",
    "        self.table_name = table_name\n",
    "        self.table_as_name = table_as_name\n",
    "        self.children = children\n",
    "\n",
    "    def get_select_part(self):\n",
    "        str = \"\"\n",
    "        # (column_name, None)\n",
    "        # (column_name, column_as_name)\n",
    "        # require translation: (column_name, (column_as_name))\n",
    "        for key in self.selected_columns:\n",
    "            value = self.selected_columns[key]\n",
    "            if isinstance(value, tuple):\n",
    "                str += \"lt_\" + value[0] + \".ltr_text as \" + value[0] + \",\\n\"\n",
    "            else:\n",
    "                if value is None:\n",
    "                    str += self.table_as_name + \".\" + key + \",\\n\"\n",
    "                else:\n",
    "                    str += self.table_as_name + \".\" + key + \" as \" + value + \",\\n\"\n",
    "        return str[:-2]\n",
    "\n",
    "    def get_from_head(self):\n",
    "        return self.table_name + \" \" + self.table_as_name + \"\\n\"\n",
    "\n",
    "    def get_from_following(self):\n",
    "        str = \"\"\n",
    "        for key in self.selected_columns:\n",
    "            value = self.selected_columns[key]\n",
    "            if isinstance(value, tuple):\n",
    "                str += \"JOIN ref_list_detail rld_\" + value[0] + \"\\n\"\n",
    "                str += \"ON \" + self.table_as_name + \".\" + key + \" = rld_\" + value[0] + \".rld_id\\n\"\n",
    "                str += \"JOIN ref_list_header rlh_\" + value[0] + \"\\n\"\n",
    "                str += \"ON rld_\" + value[0] + \".rld_rlh_id = rlh_\" + value[0] + \".rlh_id\\n\"\n",
    "                str += \"JOIN language_translation lt_\" + value[0] + \"\\n\"\n",
    "                str += \"ON rld_\" + value[0] + \".rld_name = lt_\" + value[0] + \".ltr_tag\\n\"\n",
    "        return str\n",
    "\n",
    "    def get_from_part(self):\n",
    "        return self.get_from_head() + self.get_from_following()\n",
    "\n",
    "    def get_where_part(self, add_and=False):\n",
    "        i = 0\n",
    "        str = \"\"\n",
    "        for key in self.selected_columns:\n",
    "            value = self.selected_columns[key]\n",
    "            if isinstance(value, tuple):\n",
    "                if i != 0 or add_and:\n",
    "                    str += \"and \"\n",
    "                i = i + 1\n",
    "                str += \"lt_\" + value[0] + \".ltr_tag_type = rld_\" + value[0] + \".rld_tag_type\"\n",
    "                str += \" and \"\n",
    "                str += \"lt_\" + value[0] + \".ltr_lng_id = 1\\n\"\n",
    "        return str\n",
    "\n",
    "    def to_sql(self):\n",
    "        queue = [self]\n",
    "        select_str = \"\"\n",
    "        from_str = self.get_from_part()\n",
    "        where_str = \"\"\n",
    "        i = 0\n",
    "        while len(queue) != 0:\n",
    "            node = queue.pop(0)\n",
    "            # print(node.table_name)\n",
    "            if i != 0 and len(node.selected_columns) != 0 and len(select_str) != 0:\n",
    "                select_str += ',\\n'\n",
    "            add_and = False\n",
    "            if len(where_str) != 0:\n",
    "                add_and = True\n",
    "            select_str += node.get_select_part()\n",
    "            where_str += node.get_where_part(add_and)\n",
    "            for child in node.children:\n",
    "                child_node = child[1]\n",
    "                queue.append(child_node)\n",
    "                from_str += \"JOIN \" + child_node.get_from_head()\n",
    "                from_str += \"on \" + node.table_as_name + \".\" + child[0][0] + \"=\" \\\n",
    "                            + child_node.table_as_name + \".\" + child[0][1] + \"\\n\"\n",
    "                from_str += child_node.get_from_following()\n",
    "            i += 1\n",
    "        return \"select distinct \" + select_str + \"\\nfrom \" + from_str + \"where \" + where_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9g9DYYzT_CM"
   },
   "source": [
    "After studying the database provided by Sofvie, 3 types of data are required, namely incidents, incident's type and supervisor data. \n",
    "\n",
    "The following python file generates the SQL required for the data extraction and outputs the extracted data to CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Incident Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rZfzuQ_Z_nc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_incident_sql():\n",
    "    incident_table = TableNode({'ID': 'incidentId', 'IncidentNumber': None, 'CreationDate': 'IncidentCreationDate'},\n",
    "                               'Incidents', 'incidents', {})\n",
    "    incident_submission_table = TableNode({}, 'IncidentSubmissions', 'incidentSubmissions', {})\n",
    "    submission_header_table = TableNode(\n",
    "        {'Site': ('site',),\n",
    "         'SubmittedBy_SupervisorID': 'per_id'}, 'SubmissionHeader', 'submissionHeaders', {})\n",
    "    incident_table.children = [((\"ID\", \"IncidentId\"), incident_submission_table)]\n",
    "    incident_submission_table.children = [((\"SubmissionHeaderId\", \"ID\"), submission_header_table)]\n",
    "    return incident_table.to_sql()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIGr8iYqaEdU",
    "outputId": "7877c4fa-74d1-463d-b050-b653286611ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "incident_sql = generate_incident_sql()\n",
    "print(incident_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Supervisor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBvFoehYaRQd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_supervisor_sql():\n",
    "    incident_table = TableNode({},\n",
    "                               'Incidents', 'incidents', {})\n",
    "    incident_submission_table = TableNode({}, 'IncidentSubmissions', 'incidentSubmissions', {})\n",
    "    submission_header_table = TableNode({}, 'SubmissionHeader', 'submissionHeaders', {})\n",
    "    person_table = TableNode(\n",
    "        {'per_id': None, 'per_dob': None, 'per_first_name': None, 'per_middle_name': None, 'per_last_name': None,\n",
    "         'per_sin': None,\n",
    "         'per_gender': ('per_gender',), 'per_enable': None}, 'person', 'person', {})\n",
    "    emp_table = TableNode({'emp_pos_id': ('emp_pos',), 'emp_start_date': None}, 'employee', 'emp', {})\n",
    "    emp_job_table = TableNode({'ejo_job_id': ('emp_job',)}, 'employee_job', 'emp_job', {})\n",
    "    emp_site_table = TableNode({'esi_sit_id': ('emp_site',)}, 'employee_site', 'emp_site', {})\n",
    "    '''\n",
    "    emp_training_table = TableNode({'etr_training_type_id': ('etr_training_type',),\n",
    "                                    'etr_training_institution_id': ('etr_training_institution',),\n",
    "                                    'etr_training_code_id': ('etr_training_code',), 'etr_completion_date': None,\n",
    "                                    'etr_training_status_id': ('etr_training_status',)}, 'employee_training',\n",
    "                                   'emp_train', {})\n",
    "    '''                               \n",
    "    incident_table.children = [((\"ID\", \"IncidentId\"), incident_submission_table)]\n",
    "    incident_submission_table.children = [((\"SubmissionHeaderId\", \"ID\"), submission_header_table)]\n",
    "    submission_header_table.children = [((\"SubmittedBy_SupervisorID\", \"per_id\"), person_table)]\n",
    "    person_table.children = [((\"per_id\", \"emp_per_id\"), emp_table)]\n",
    "    emp_table.children = [((\"emp_id\", \"ejo_emp_id\"), emp_job_table), (\n",
    "        (\"emp_id\", \"esi_emp_id\"), emp_site_table)]  # , (\"emp_id\", \"etr_emp_id\"): emp_training_table}\n",
    "    return incident_table.to_sql()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fe_qOG07aJsS",
    "outputId": "7b861fb6-38b3-4696-e4ff-cf51c1a1de65",
    "tags": []
   },
   "outputs": [],
   "source": [
    "supervisor_sql = generate_supervisor_sql()\n",
    "print(supervisor_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_wIpdu6adfP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_incident_type_sql():\n",
    "    incident_table = TableNode({'ID': 'incidentId'},\n",
    "                               'Incidents', 'incidents', {})\n",
    "    root_cause_analysis_table = TableNode({'PreliminaryTypeId': ('PreliminaryType',),\n",
    "                                           'PotentialLossID': ('PotentialLoss',),\n",
    "                                           'ActualTypeId': ('actualType',),\n",
    "                                           'IncidentTypeId': ('incidentType',)},\n",
    "                                          'RootCauseAnalysis', 'rca', {})\n",
    "    incident_table.children = [((\"ID\", \"IncidentId\"), root_cause_analysis_table)]\n",
    "    return incident_table.to_sql()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Incident Type Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JeJRlWWSaiuo",
    "outputId": "84cbce78-0d8f-4ee4-c287-fb71d96c0348",
    "tags": []
   },
   "outputs": [],
   "source": [
    "incident_type_sql = generate_incident_type_sql()\n",
    "print(incident_type_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Export the required data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HM6cLpUlaqS_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = database_connection.run_sql(incident_sql)\n",
    "data.to_csv('resources/incidents.csv', encoding='utf-8', index=False)\n",
    "data = database_connection.run_sql(incident_type_sql)\n",
    "data.to_csv('resources/incidents_type.csv', encoding='utf-8', index=False)\n",
    "data = database_connection.run_sql(supervisor_sql)\n",
    "data.to_csv('resources/supervisor.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5negqvoxbYoi"
   },
   "source": [
    "# **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nc4tY7OeeMyn"
   },
   "source": [
    "## **plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-er2Xp62eT_s"
   },
   "outputs": [],
   "source": [
    "def plot_data_distribution(data, label, title, xlabel, ylabel, figsize=(10, 5)):\n",
    "    counts = data[label].value_counts()\n",
    "    # create bar chart\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.bar(counts.index, counts.values)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(ylabel)\n",
    "    ax.set_ylabel(xlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8-E5muThjHN"
   },
   "outputs": [],
   "source": [
    "def boxplot(data, column_name, label, title, xlabel, ylabel, figsize=(10, 5)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    categories = data[label].unique()\n",
    "    values = [data[data[label] == c][column_name] for c in categories]\n",
    "    ax.boxplot(values)\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjcWjf-seUrv"
   },
   "source": [
    "## **data cleaning and concatenation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRj06fEaUSLN"
   },
   "source": [
    "As the raw data extracted had dirty data and unstandardized values, some data cleaning was performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnRuRaKtbw14"
   },
   "outputs": [],
   "source": [
    "def aggregate_to_list(x, remove_words=[]):\n",
    "    lst = []\n",
    "    for i in x:\n",
    "        removeI = False\n",
    "        for r in remove_words:\n",
    "            if r in i:\n",
    "                removeI = True\n",
    "                break\n",
    "        if i not in lst and not removeI:\n",
    "            lst.append(i)\n",
    "    return ', '.join([str(i) for i in lst])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftZIqop4bX1S"
   },
   "outputs": [],
   "source": [
    "def clean_supervisor_data(supervisor_data):\n",
    "    supervisor_data.drop(['per_sin', 'per_gender'], axis=1)\n",
    "    emp_pos_dict = {\n",
    "        'Safety and Training Supervisor': 'Safety Supervisor, Training Supervisor',\n",
    "        'Safety Supervisor 01': 'Safety Supervisor',\n",
    "        'Safety Supervisor 02': 'Safety Supervisor',\n",
    "        'Junior Miner B': 'Miner',\n",
    "        'Mid Miner B': 'Miner',\n",
    "        'Mid Miner C': 'Miner',\n",
    "        'Construction Miner C': 'Miner'\n",
    "    }\n",
    "    supervisor_data[\"emp_pos\"] = supervisor_data[\"emp_pos\"].apply(\n",
    "        lambda x: emp_pos_dict[x] if x in emp_pos_dict.keys() else x)\n",
    "    supervisor_data = supervisor_data.groupby('per_id').agg({\n",
    "        'per_dob': 'first',\n",
    "        'per_first_name': 'first',\n",
    "        'per_middle_name': 'first',\n",
    "        'per_last_name': 'first',\n",
    "        'emp_start_date': 'first',\n",
    "        'per_enable': lambda x: aggregate_to_list(x),\n",
    "        'emp_pos': lambda x: aggregate_to_list(x),\n",
    "        'emp_site': lambda x: aggregate_to_list(x)\n",
    "    })\n",
    "    return supervisor_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCeX7JMVb1aG"
   },
   "outputs": [],
   "source": [
    "def get_cleaned_supervisor_data():\n",
    "    supervisor_data = pd.read_csv(\"resources/supervisor.csv\")\n",
    "    supervisor_data = clean_supervisor_data(supervisor_data)\n",
    "    return supervisor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hH6EcgE8b37E"
   },
   "outputs": [],
   "source": [
    "def get_cleaned_incident_data():\n",
    "    incidents_data = pd.read_csv(\"resources/incidents.csv\")\n",
    "    return incidents_data\n",
    "\n",
    "\n",
    "def get_cleaned_incident_type_data():\n",
    "    incidents_type_data = pd.read_csv(\"resources/incidents_type.csv\")\n",
    "    return incidents_type_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aB0ruCeKUbdA"
   },
   "source": [
    "The data is then combined by joining the incident data with the incident type based on the `incidentId`. The supervisor data is then joined to the data by the `per_id` column.\n",
    "\n",
    "The resulting data is output to a CSV file `cleaned_incident_data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8gB-VBRcA4J",
    "outputId": "b4f6b2d0-18f9-4f77-9a85-8c5eb296210e"
   },
   "outputs": [],
   "source": [
    "cleaned_supervisor_data = get_cleaned_supervisor_data()\n",
    "cleaned_incident_data = get_cleaned_incident_data()\n",
    "cleaned_incident_type_data = get_cleaned_incident_type_data()\n",
    "cleaned_incident_data['incidentId'] = cleaned_incident_data['incidentId'].astype(int)\n",
    "cleaned_incident_type_data['incidentId'] = cleaned_incident_type_data['incidentId'].astype(int)\n",
    "cleaned_incident_data = cleaned_incident_data.set_index('incidentId').join(cleaned_incident_type_data.set_index('incidentId'), on='incidentId', how=\"inner\")\n",
    "cleaned_incident_data = cleaned_incident_data.join(cleaned_supervisor_data, on='per_id', how=\"inner\", lsuffix='l_', rsuffix='r_')\n",
    "print(cleaned_incident_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo_wI3luoS24"
   },
   "source": [
    "## **data augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zD2t6hDtUqws"
   },
   "source": [
    "As seen below, the available data is unbalanced in terms of the number of incidents for each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "yDW0XDDqefm2",
    "outputId": "3e7c7b28-d9fb-4358-9fc6-01cc6b0170d1"
   },
   "outputs": [],
   "source": [
    "plot_data_distribution(cleaned_incident_data, 'incidentType', 'data distribution', 'incident type', 'number of data', (15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4IhEo0PUwWF"
   },
   "source": [
    "As such, classification is only done on the following classes `A`, `B`, `C`, `HPI` due to the unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQiASKM1gexV"
   },
   "outputs": [],
   "source": [
    "def select_type(incidents, type_list=[]):\n",
    "    return incidents[incidents['incidentType'].isin(type_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "QFTCeIYtgf_P",
    "outputId": "a825525f-df10-44fb-8f87-b228b5150f31"
   },
   "outputs": [],
   "source": [
    "cleaned_incident_data = select_type(cleaned_incident_data, ['A', 'B', 'C', 'HPI'])\n",
    "plot_data_distribution(cleaned_incident_data, 'incidentType', 'data distribution', 'incident type', 'number of data', (15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTSUkqX4U6B5"
   },
   "source": [
    "To aide in balancing the dataset, data augmentation is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iS9bx-Xugmsa"
   },
   "outputs": [],
   "source": [
    "def data_augmentation(incidents_data):\n",
    "    counts = incidents_data['incidentType'].value_counts()\n",
    "    duplicated_data = []\n",
    "    for label in counts.index:\n",
    "        label_data = incidents_data[incidents_data['incidentType'] == label]\n",
    "        n_samples = int(0.9 * (max(counts) - len(label_data)))\n",
    "        if n_samples > 0:\n",
    "            for i in range(n_samples):\n",
    "                duplicated_data.append(label_data.iloc[random.randint(0, len(label_data)) % len(label_data)])\n",
    "    augmented_df = pd.concat([incidents_data, pd.DataFrame(duplicated_data)], ignore_index=True)\n",
    "    augmented_df = augmented_df.sample(frac=1)\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "MVClxWFBgqHj",
    "outputId": "2a75f6a2-eaea-4b08-afb6-6fa3ec50ea11"
   },
   "outputs": [],
   "source": [
    "cleaned_incident_data = data_augmentation(cleaned_incident_data)\n",
    "plot_data_distribution(cleaned_incident_data, 'incidentType', 'data distribution after data augmentation', 'incident type', 'number of data', (15, 5))\n",
    "cleaned_incident_data.to_csv(\"resources/cleaned_incident_data.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtkBJTJ_TpAa"
   },
   "outputs": [],
   "source": [
    "database_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiZ4x056oh8g"
   },
   "source": [
    "# **feature engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The resulting data is further enhanced for the training set with the aim of enhancing model accuracy.\n",
    "\n",
    "The following features were transformed and/or normalized in the steps below:\n",
    "- `IncidentCreationDateTimestamp`: normalization of timeseries data to a standard format\n",
    "- `IncidentCreationYear`: represent incident creation year to a standard timestamp year format\n",
    "- `IncidentCreationMonth`: represent incident creation year to a standard timestamp year format\n",
    "- `IncidentCreationWeekday`: represent incident creation year to a standard timestamp weekday format\n",
    "- `per_dob_timestamp`: normalization of timeseries data to a standard format\n",
    "- `emp_start_date_timestamp`: normalization of timeseries data to a standard format\n",
    "- `per_middle_name`: replace `N/A` values with empty string `\"\"`\n",
    "- `per_name`: combines first name (`per_first_name`), middle name (`per_middle_name`) and last name (`per_last_name`) of employee.\n",
    "\n",
    "The following new features were introduced in the steps below:\n",
    "- `incidentDobDayBetween`: number of days between incident date and employee's date of birth\n",
    "- `incidentEmpStartDayBetween` number of days beteeen incident date and employee's start date\n",
    "- `DobEmpStartDayBetween`: number of days between employee's start date and date of birth\n",
    "- `emp_pos_cnt`: number of positions the employee holds\n",
    "- `emp_site_cnt`: number of sites the employee is assigned to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZAmLz4Vqf7Hi"
   },
   "outputs": [],
   "source": [
    "incident_data = cleaned_incident_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Re1rEtfefw04"
   },
   "outputs": [],
   "source": [
    "incident_data['IncidentCreationDateTimestamp'] = pd.to_datetime(incident_data['IncidentCreationDate'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "incident_data['per_dob_timestamp'] = pd.to_datetime(incident_data['per_dob'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "incident_data['emp_start_date_timestamp'] = pd.to_datetime(incident_data['emp_start_date'], format=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXj6ZP4YgIU3"
   },
   "outputs": [],
   "source": [
    "incident_data['IncidentCreationYear'] = incident_data['IncidentCreationDateTimestamp'].dt.year\n",
    "incident_data['IncidentCreationMonth'] = incident_data['IncidentCreationDateTimestamp'].dt.month\n",
    "incident_data['IncidentCreationWeekday'] = incident_data['IncidentCreationDateTimestamp'].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyWaAaErVTj2"
   },
   "source": [
    "(to comment: distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "r2vhKkKQh1NY",
    "outputId": "919e0752-591d-48dc-9d75-807afdd21f9a"
   },
   "outputs": [],
   "source": [
    "boxplot(incident_data, 'IncidentCreationYear', 'incidentType', 'boxplot of incident creation year', 'incidentType', 'creation year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_h1Jmg1RVYaD"
   },
   "source": [
    "(to comment: distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "u7kqgOtmim-A",
    "outputId": "b645ab6d-c025-4390-8cdc-0cf29b0c06a5"
   },
   "outputs": [],
   "source": [
    "boxplot(incident_data, 'IncidentCreationMonth', 'incidentType', 'boxplot of incident creation Month', 'incidentType', 'creation month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnMQrLBzVZJM"
   },
   "source": [
    "(to comment: distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "lK1pWGMOiv4k",
    "outputId": "a49827c2-654d-40c1-a479-bab6625b5764"
   },
   "outputs": [],
   "source": [
    "boxplot(incident_data, 'IncidentCreationWeekday', 'incidentType', 'boxplot of incident creation Weekday', 'incidentType', 'creation weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbCEcBn_gOOX"
   },
   "outputs": [],
   "source": [
    "incident_data['incidentDobDayBetween'] = (incident_data[\"IncidentCreationDateTimestamp\"] - incident_data['per_dob_timestamp']).dt.days\n",
    "incident_data['incidentEmpStartDayBetween'] = (incident_data[\"IncidentCreationDateTimestamp\"] - incident_data['emp_start_date_timestamp']).dt.days\n",
    "incident_data['DobEmpStartDayBetween'] = (incident_data[\"emp_start_date_timestamp\"] - incident_data['per_dob_timestamp']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eSdHWAcV3f5"
   },
   "source": [
    "(to comment: distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "1XXAYHDLi_gX",
    "outputId": "6108e9d7-768d-4974-8b04-65741b2618ec"
   },
   "outputs": [],
   "source": [
    "boxplot(incident_data, 'incidentDobDayBetween', 'incidentType', 'boxplot of day between dob and incident creation date', 'incidentType', 'days between')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS_V2Um1WEs_"
   },
   "source": [
    "(to comment: distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "5LtS_cc9jUpZ",
    "outputId": "f7db59a6-379e-415b-c9f5-3b1e4d00a6a8"
   },
   "outputs": [],
   "source": [
    "boxplot(incident_data, 'incidentEmpStartDayBetween', 'incidentType', 'boxplot of day between employee start date and incident creation date', 'incidentType', 'days between')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CcabyLNWFZ6"
   },
   "source": [
    "(to comment: distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "FMci4GYrjg8X",
    "outputId": "e8c9f7dd-9031-4727-e4ac-3c6bc92038d3"
   },
   "outputs": [],
   "source": [
    "boxplot(incident_data, 'DobEmpStartDayBetween', 'incidentType', 'boxplot of day between employee start date and dob', 'incidentType', 'days between')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6iA2iDzfp5a"
   },
   "outputs": [],
   "source": [
    "incident_data['per_middle_name'] = incident_data['per_middle_name'].fillna(\"\")\n",
    "incident_data['per_name'] = incident_data['per_first_name'] + \" \" + incident_data['per_middle_name'] + \" \" + incident_data['per_last_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuTNnNYoj1SC"
   },
   "outputs": [],
   "source": [
    "incident_data['emp_pos_cnt'] = incident_data['emp_pos'].apply(lambda x: len(str(x).split(',')) if x is not None else 1)\n",
    "incident_data['emp_site_cnt'] = incident_data['emp_site'].apply(lambda x: len(str(x).split(',')) if x is not None else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35P4XcR_WTMQ"
   },
   "source": [
    "(to comment: distribution and not using this column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "Stu95vwKkEEL",
    "outputId": "ffed1149-cebf-43fb-89f4-7fdf47b74065"
   },
   "outputs": [],
   "source": [
    "boxplot(incident_data, 'emp_pos_cnt', 'incidentType', 'boxplot of employee position count', 'incidentType', 'employee position count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDvPYa14WZII"
   },
   "source": [
    "(to comment: distribution and not using this column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "7E0AEslaly25",
    "outputId": "17f20224-be1f-4173-ce80-61f914685dee"
   },
   "outputs": [],
   "source": [
    "boxplot(incident_data, 'emp_site_cnt', 'incidentType', 'boxplot of employee site count', 'incidentType', 'employee site count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBqJHUdMWaBn"
   },
   "source": [
    "(to comment: why not one hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUHtofubl5aW",
    "outputId": "9cb8246a-dd28-4b4e-eb8b-a010f2b6f42d"
   },
   "outputs": [],
   "source": [
    "site_one_hot = pd.get_dummies(incident_data[\"site\"])\n",
    "print(site_one_hot.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tywbeyXAWeH6"
   },
   "source": [
    "(to comment: frequency encoding on site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5yzi7wSm39n"
   },
   "outputs": [],
   "source": [
    "site_cnt = incident_data['site'].value_counts()\n",
    "incident_data['site_freq'] = incident_data['site'].apply(lambda x: site_cnt[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ShM-RSmWigS"
   },
   "source": [
    "(to comment: distribution of site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "0B3KjWTMmFXb",
    "outputId": "6f7c7ea5-3f33-4291-9df9-3b4fd32dd43e"
   },
   "outputs": [],
   "source": [
    "boxplot(incident_data, 'site_freq', 'incidentType', 'boxplot of site frequency encoding', 'incidentType', 'incident site frequency encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ut459DK4n8nZ"
   },
   "outputs": [],
   "source": [
    "incident_data = incident_data.drop(['IncidentNumber', 'IncidentCreationDate', 'site', 'per_id', 'PotentialLoss', 'PreliminaryType', 'actualType',\n",
    "                    'per_dob', 'per_first_name', 'per_middle_name', 'per_last_name', 'emp_start_date', 'emp_pos', 'emp_site',\n",
    "                    'IncidentCreationDateTimestamp', 'per_dob_timestamp', 'emp_start_date_timestamp'], axis=1)\n",
    "incident_data = incident_data.drop_duplicates()\n",
    "incident_data.to_csv(\"resources/incident_data.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoqiR8qqu4Fh"
   },
   "source": [
    "# **Model construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmdkTAS3D5Pw"
   },
   "outputs": [],
   "source": [
    "def k_fold_train(model, X, y, n_splits=5):\n",
    "    # k-fold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # average accuracy\n",
    "    avg_acc = 0\n",
    "    # average precesion\n",
    "    avg_pre = 0\n",
    "    # average recall\n",
    "    avg_rec = 0\n",
    "    # average f1 score\n",
    "    avg_f1 = 0\n",
    "    # iterate over k-fold\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # scale data\n",
    "        X_train, X_test = model.normalize(X_train, X_test)\n",
    "        # create a new model\n",
    "        ml_model = model.get_model()\n",
    "        ml_model.fit(X_train, y_train)\n",
    "        y_pred = ml_model.predict(X_test)\n",
    "        # calcualte precision, recall, accuracy, f1-score and confusion matrix on this fold\n",
    "        precision = precision_score(y_test, y_pred, average='micro')\n",
    "        recall = recall_score(y_test, y_pred, average='micro' )\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='micro')\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        # add to average value\n",
    "        avg_acc = avg_acc + accuracy\n",
    "        avg_pre = avg_pre + precision\n",
    "        avg_rec = avg_rec + recall\n",
    "        avg_f1 = avg_f1 + f1\n",
    "        model.save(accuracy)\n",
    "        print(f'Fold {i + 1}:')\n",
    "        print(f'Precision = {precision:.3f}')\n",
    "        print(f'Recall = {recall:.3f}')\n",
    "        print(f'Accuracy = {accuracy:.3f}')\n",
    "        print(f'F1-score = {f1:.3f}')\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.labels,\n",
    "              yticklabels=model.labels)\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.ylabel('True label')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "    model.add_result({\n",
    "        'acc': avg_acc / n_splits,\n",
    "        'pre': avg_pre / n_splits,\n",
    "        'rec': avg_rec / n_splits,\n",
    "        'f1': avg_f1 / n_splits\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7pZ9rNpEOXp"
   },
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, hyperparam_dicts, model_output_path='resources',\n",
    "                 performance_lst=pd.DataFrame(columns=['model', 'hyperparam', 'accuracy', 'precision', 'recall', 'f1-score'])):\n",
    "        self.best_acc = 0\n",
    "        self.hyperparam_dicts = hyperparam_dicts\n",
    "        self.model = None\n",
    "        self.hyperparam_dict = None\n",
    "        self.model_output_path = model_output_path\n",
    "        self.performance_lst = performance_lst\n",
    "\n",
    "    # select relative columns to train the model\n",
    "    def select_columns(self, incident_data):\n",
    "        X = incident_data.drop(columns=['incidentType', 'IncidentCreationYear', 'per_enable', 'per_name', 'emp_pos_cnt', 'emp_site_cnt'])\n",
    "        y = incident_data['incidentType']\n",
    "        self.labels = y.unique()\n",
    "        return X.to_numpy(), y.to_numpy()\n",
    "\n",
    "    # initialize a new model based on hyperparameter\n",
    "    def create_instance(self, hyperparam_dict):\n",
    "        criterion = 'entropy'\n",
    "        n_estimators = 20\n",
    "        max_depth = 4\n",
    "        min_samples_leaf = 1\n",
    "        if 'critertion' in hyperparam_dict:\n",
    "            criterion = hyperparam_dict['critertion']\n",
    "        else:\n",
    "            hyperparam_dict['critertion'] = criterion\n",
    "        if 'n_estimators' in hyperparam_dict:\n",
    "            n_estimators = hyperparam_dict['n_estimators']\n",
    "        else:\n",
    "            hyperparam_dict['n_estimators'] = n_estimators\n",
    "        if 'max_depth' in hyperparam_dict:\n",
    "            max_depth = hyperparam_dict['max_depth']\n",
    "        else:\n",
    "            hyperparam_dict['max_depth'] = max_depth\n",
    "        if 'min_sample_leaf' in hyperparam_dict:\n",
    "            min_samples_leaf = hyperparam_dict['min_samples_leaf']\n",
    "        else:\n",
    "            hyperparam_dict['min_samples_leaf'] = min_samples_leaf\n",
    "        self.hyperparam_dict = hyperparam_dict\n",
    "        self.model = RandomForestClassifier(criterion=criterion, n_estimators=n_estimators,\n",
    "                                            max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "        return self.model\n",
    "\n",
    "    # normalization(random forest not needed)\n",
    "    def normalize(self, X_train, X_test):\n",
    "        return X_train, X_test\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    # train models based different hyperparameters with k-fold cross validation\n",
    "    def train(self, data):\n",
    "        X, y = self.select_columns(data)\n",
    "        for hyperparam_dict in self.hyperparam_dicts:\n",
    "            print(\"===========================================================================\")\n",
    "            print(\"training for hyperparameter: \" + str(self.hyperparam_dict) + \"\\n\")\n",
    "            self.model = self.create_instance(hyperparam_dict)\n",
    "            k_fold_train(self, X, y)\n",
    "\n",
    "    # add the averaged result for one model(one hyperparameter) into performance list\n",
    "    def add_result(self, res):\n",
    "        self.performance_lst = pd.concat([self.performance_lst, pd.DataFrame([{\n",
    "            'model': 'random forest',\n",
    "            'hyperparam': str(self.hyperparam_dict),\n",
    "            'accuracy': res['acc'],\n",
    "            'precision': res['pre'],\n",
    "            'recall': res['rec'],\n",
    "            'f1-score': res['f1']\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    def get_performance_lst(self):\n",
    "      return self.performance_lst\n",
    "\n",
    "    # if the model has a beter accuracy, save the model (and the scaler if appiciable)\n",
    "    def save(self, acc):\n",
    "        if (acc > self.best_acc):\n",
    "            self.best_acc = acc\n",
    "            joblib.dump(self.model, os.path.join(self.model_output_path, 'model_rf.pkl'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49_hYadNYvkg"
   },
   "source": [
    "(to comment: result and model not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BYhhaNFMEWcJ",
    "outputId": "c207ed26-0a09-457b-95ea-964cf11843f8"
   },
   "outputs": [],
   "source": [
    "randomForest = RandomForest([{}, \n",
    "                {'n_estimators': 50},\n",
    "                {'n_estimators': 100}\n",
    "                ])\n",
    "randomForest.train(incident_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YElFvFKxFQb7",
    "outputId": "0792bce6-d729-47c9-8323-340280e2e72d"
   },
   "outputs": [],
   "source": [
    "performance_lst = randomForest.get_performance_lst()\n",
    "print(performance_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJV9cXwGQtWe"
   },
   "outputs": [],
   "source": [
    "class ModelSVM:\n",
    "    def __init__(self, hyperparam_dicts, model_output_path='resources',\n",
    "                 performance_lst=pd.DataFrame(columns=['model', 'hyperparam', 'accuracy', 'precision', 'recall', 'f1-score'])):\n",
    "        self.best_acc = 0\n",
    "        self.hyperparam_dicts = hyperparam_dicts\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.hyperparam_dict = None\n",
    "        self.model_output_path = model_output_path\n",
    "        self.performance_lst = performance_lst\n",
    "\n",
    "    # select relative columns to train the model\n",
    "    def select_columns(self, incident_data):\n",
    "        X = incident_data.drop(columns=['incidentType', 'IncidentCreationYear', 'per_enable', 'per_name', 'emp_pos_cnt', 'emp_site_cnt'])\n",
    "        y = incident_data['incidentType']\n",
    "        self.labels = y.unique()\n",
    "        return X.to_numpy(), y.to_numpy()\n",
    "\n",
    "    # initialize a new model based on hyperparameter\n",
    "    def create_instance(self, hyperparam_dict):\n",
    "        kernel = 'rbf'\n",
    "        C = 1.0\n",
    "        gamma = 'scale'\n",
    "        if 'kernel' in hyperparam_dict:\n",
    "            kernel = hyperparam_dict['kernel']\n",
    "        else:\n",
    "            hyperparam_dict['kernel'] = kernel\n",
    "        if 'C' in hyperparam_dict:\n",
    "            C = hyperparam_dict['C']\n",
    "        else:\n",
    "            hyperparam_dict['C'] = C\n",
    "        if 'gamma' in hyperparam_dict:\n",
    "            gamma = hyperparam_dict['gamma']\n",
    "        else:\n",
    "            hyperparam_dict['gamma'] = gamma\n",
    "        self.hyperparam_dict = hyperparam_dict\n",
    "        self.model = SVC(kernel=kernel, C=C, gamma=gamma)\n",
    "        return self.model\n",
    "\n",
    "    # normalization(stnd scaler)\n",
    "    def normalize(self, X_train, X_test):\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        return X_train, X_test\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    # train models based different hyperparameters with k-fold cross validation\n",
    "    def train(self, data):\n",
    "        X, y = self.select_columns(data)\n",
    "        for hyperparam_dict in self.hyperparam_dicts:\n",
    "            print(\"===========================================================================\")\n",
    "            print(\"training for hyperparameter: \" + str(self.hyperparam_dict) + \"\\n\")\n",
    "            self.model = self.create_instance(hyperparam_dict)\n",
    "            k_fold_train(self, X, y)\n",
    "\n",
    "    # add the averaged result for one model(one hyperparameter) into performance list\n",
    "    def add_result(self, res):\n",
    "        self.performance_lst = pd.concat([self.performance_lst, pd.DataFrame([{\n",
    "            'model': 'SVM',\n",
    "            'hyperparam': str(self.hyperparam_dict),\n",
    "            'accuracy': res['acc'],\n",
    "            'precision': res['pre'],\n",
    "            'recall': res['rec'],\n",
    "            'f1-score': res['f1']\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    def get_performance_lst(self):\n",
    "      return self.performance_lst\n",
    "\n",
    "    # if the model has a beter accuracy, save the model and the scaler\n",
    "    def save(self, acc):\n",
    "        if (acc > self.best_acc):\n",
    "            self.best_acc = acc\n",
    "            joblib.dump(self.scaler, os.path.join(self.model_output_path, 'scaler_svm.pkl'))\n",
    "            joblib.dump(self.model, os.path.join(self.model_output_path, 'model_svm.pkl'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vka26i3qY8Ur"
   },
   "source": [
    "(to comment: result and model not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bRzO9DKMQ1FI",
    "outputId": "f9974b3f-6fd8-48b6-a5c5-efd5cbc5ca4c"
   },
   "outputs": [],
   "source": [
    "svm = ModelSVM([{'kernel': 'rbf'}, \n",
    "          {'kernel': 'linear'},\n",
    "          {'kernel': 'poly'}\n",
    "         ],\n",
    "         performance_lst=performance_lst)\n",
    "svm.train(incident_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRGJvv3pSAdK",
    "outputId": "aaf138c3-dc64-47c9-bec5-e65255953f7e"
   },
   "outputs": [],
   "source": [
    "performance_lst = svm.get_performance_lst()\n",
    "print(performance_lst)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
